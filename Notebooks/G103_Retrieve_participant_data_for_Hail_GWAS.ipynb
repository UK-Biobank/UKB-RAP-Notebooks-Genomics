{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve participant data for Hail GWAS\n",
    "\n",
    "- runtime: 10min \n",
    "- recommended instance: mem1_ssd1_v2_x8\n",
    "- cost: <Â£0.10\n",
    "\n",
    "This notebook depends on:\n",
    "* **A Spark instance**\n",
    "\n",
    "In this notebook, we will access phenotypic data stored in the Spark database.\n",
    "\n",
    "The data is saved as a csv file and uploaded onto the project in a folder called *pheno*.\n",
    "For the sole purpose of illustrating GWAS in Hail, field 1239 (*Current tobacco smoking*) was considered, this can be modified based on the trait you are interested in.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import `dxdata` package and initialize Spark engine\n",
    "### Docs at: https://github.com/dnanexus/OpenBio/blob/master/dxdata/getting_started_with_dxdata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dxdata\n",
    "import os\n",
    "\n",
    "# Initialize dxdata engine\n",
    "engine = dxdata.connect(dialect=\"hive+pyspark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the dataset\n",
    "\n",
    "Next, we can set a `DATASET_ID` variable, which takes a value: `[projectID]:[dataset ID]`\n",
    "We use it to define the `dataset` with `dxdata.load_dataset` function.\n",
    "\n",
    "**projectID** and **dataset ID** values are unique to your project.\n",
    "Notebook example **[A101](https://github.com/UK-Biobank/UKB-RAP-Notebooks-Access/blob/main/JupyterNotebook_Python/A101_Explore-phenotype-tables_Python.ipynb)** explains how to get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project = os.popen(\"dx env | grep project- | awk -F '\\t' '{print $2}'\").read().rstrip()\n",
    "record = os.popen(\"dx describe *dataset | grep  record- | awk -F ' ' '{print $2}'\").read().rstrip().split('\\n')[0]\n",
    "DATASET_ID = project + \":\" + record\n",
    "dataset = dxdata.load_dataset(id=DATASET_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from the table\n",
    "\n",
    "The following code selects the `participant` table.\n",
    "Then we can define which field we are interested in using the `find_field` function.\n",
    "For illustration, we are looking at field 1239 - whether a participant smokes (considering instance 0 - at baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pheno = dataset['participant']\n",
    "\n",
    "# Find by field name & title\n",
    "field_eid = pheno.find_field(name=\"eid\")\n",
    "smk = pheno.find_field(title=\"Current tobacco smoking | Instance 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_list = [field_eid, smk]\n",
    "field_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data from the Spark DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pheno_data = pheno.retrieve_fields(engine=engine, fields=field_list, coding_values=\"replace\")\n",
    "pheno_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_tab = pheno_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the data a binary split\n",
    "This is in preparation for a basic outline of performing GWAS in Hail - we are not too concerned about providing high quality data (only that it is acceptably formatted for the employed GWAS model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tab.p1239_i0[data_tab.p1239_i0 != 'No'] = 'true'\n",
    "data_tab.p1239_i0[data_tab.p1239_i0 == 'No'] = 'false'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the column names\n",
    "data_tab.columns = ['eid', 'smoking']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is saved as a csv file.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_tab.to_csv('smoking_bool.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"pheno\" folder is created in the project space to upload the csv file created. \n",
    "%%bash\n",
    "dx mkdir pheno\n",
    "dx upload smok* --path pheno/ # the grep command should be changed based on your file name. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "be0617d24f23f3f0ff0f78cfff875dd0cc8ce9ddccca39efd47dcbfb80ba815b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
